{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils_training_new_code.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Pt8JyTi276eB"},"outputs":[],"source":["import numpy as np\n","import torch\n","import os\n","import time as time\n","import pdb\n","from torch.distributions.multivariate_normal import MultivariateNormal\n","from torch.distributions import Normal\n","import visualize_new_code as viz\n","import os\n","import shutil\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import humanize\n","import psutil\n","import GPUtil\n","import pandas as pd\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class IResNet_training_on_graph():\n","    def __init__(self, models, mod_args, train_args, data_args, X_train, Y_train, X_test=None, Y_test=None):\n","        self.model, self.gen_net = models\n","        self.dim, self.nblocks = mod_args\n","        self.optimizer, self.optim_classify, self.mu, self.gamma, self.epochs, self.scheduler, self.resume_checkpoint = train_args\n","        self.edge_index, self.edge_weight, self.batch_size, self.data_name, self.num_viz, self.num_to_plot = data_args\n","        self.X_train, self.Y_train, self.X_test, self.Y_test = X_train, Y_train, X_test, Y_test\n","        self.N, self.V, self.C = self.X_train.shape\n","        # NOTE, these parameters are only updated when we access trained models afterwards for different visualizations\n","        # self.viz determines if transport cost is visualized\n","        self.cpu_load, self.viz = False, False\n","        # The criterion at which we stop training (default is <=0.01% error in consecutive training generative loss)\n","        self.stop_criterion = 1e-4\n","        # This is true ONLY when we visualize at random times our trained model\n","        self.final_viz = False\n","        self.plot_sub = True\n","        self.prefix = ''  # For file saving\n","        self.CINN_obj = ''  # For CINN training\n","        # Record transport cost over blocks at the end of .gif trajectory\n","        self.transport_cost_XtoH_ls, self.transport_cost_HtoX_ls = [], []\n","        # Sometimes we try different Y by fixing them in training, in which case it is the best to not save the reindex\n","        self.save_reindex = True if self.num_viz > 1 else False\n","        self.non_invertible_ls = []\n","    '''Train IResNet (our method)'''\n","\n","    def all_together(self):\n","        self.load_from_checkpoint()  # Load previous models from file\n","        self.get_H_cond_Y()  # Get H|Y\n","        while self.epoch < self.epochs:\n","            if device.type == 'cuda':\n","                # Useful to avoid GPU allocation excess\n","                torch.cuda.empty_cache()\n","            # Check inversion\n","            print(f\"LR is {self.optimizer.param_groups[0]['lr']}\")\n","            self.check_model_inversion()\n","            # Visualize generation\n","            self.viz_generation()\n","            if self.Y_test is not None:\n","                self.viz_generation(viz_train=False)\n","            start_epoch = time.time()\n","            print(f'Epoch {self.epoch}')\n","            loss_g_ave, loss_c_ave, classify_error_ave = self.batch_training(\n","                train=True)\n","            mem_report()  # Print GPU usage and availability\n","            self.loss_g_ls_train.append(loss_g_ave)\n","            self.loss_c_ls_train.append(loss_c_ave)\n","            if len(self.loss_g_ls_train) > 10 and np.abs((self.loss_g_ls_train[-1]-self.loss_g_ls_train[-2])/self.loss_g_ls_train[-2]) < self.stop_criterion:\n","                # If consecutive dec. less than X%, then just break.\n","                break\n","            self.classify_error_ls_train.append(classify_error_ave)\n","            if self.Y_test is not None:\n","                with torch.no_grad():\n","                    # NOTE, this is important, as o/w somehow memory accumulates and GPU depletes quickly\n","                    loss_g_ave, loss_c_ave, classify_error_ave = self.batch_training(\n","                        train=False)\n","            else:\n","                loss_g_ave, loss_c_ave, classify_error_ave = 0, 0, 1\n","            self.loss_g_ls_test.append(loss_g_ave)\n","            self.loss_c_ls_test.append(loss_c_ave)\n","            self.classify_error_ls_test.append(classify_error_ave)\n","            print(\n","                f'After Epoch {self.epoch}: \\n Training loss_g is {self.loss_g_ls_train[-1]} & Test loss_g is {self.loss_g_ls_test[-1]} \\n Training loss_c is {self.loss_c_ls_train[-1]} & Test loss_c is {self.loss_c_ls_test[-1]} \\n Training classify error is {self.classify_error_ls_train[-1]} & Test classify error is {self.classify_error_ls_test[-1]}')\n","            print(f'Epoch {self.epoch} takes {time.time()-start_epoch} secs.')\n","            self.viz_losses()\n","            self.save_checkpoint()\n","            self.epoch += 1\n","\n","    def batch_training(self, train=True):\n","        if train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        loss_g_tot = 0\n","        loss_c_tot = 0\n","        numcorrect_tot = 0\n","        N_tmp = len(Y)\n","        batch_idxs = np.arange(N_tmp)\n","        for batch in range(int(np.ceil(N_tmp / self.batch_size))):\n","            batch_idx = batch_idxs[batch\n","                                   * self.batch_size:np.min([(batch + 1) * self.batch_size, N_tmp])]\n","            loss_g = self.get_L_g(batch_idx, X, Y)\n","            if batch == 0:\n","                mem_report()  # Print GPU usage and availability\n","            loss_g_tot += loss_g\n","            if self.optim_classify:\n","                # Feed in a tensor of shape N-by-nC\n","                loss_c, numcorrect = self.get_L_c(batch_idx, X, Y)\n","            else:\n","                loss_c = torch.zeros(1).to(device)\n","                numcorrect = torch.zeros(1).to(device)\n","            numcorrect_tot += numcorrect\n","            loss_c_tot += loss_c\n","            if train:\n","                self.optimizer.zero_grad()\n","                loss_g_pls_c = loss_g + loss_c\n","                loss_g_pls_c.backward()\n","                self.optimizer.step()\n","                self.get_H_cond_Y()  # This is necessary especially when gen_net is trained, since somethings seems to be freed during training\n","                print(\n","                    f'After Epoch {self.epoch} & Batch {batch}: \\n Training loss_g is {loss_g} \\n Training loss_c is {loss_c}.')\n","        if self.scheduler is not None:\n","            self.scheduler.step()  # Need to pass in a metric\n","        if 'moon' in self.path:\n","            decay_freq = 5\n","            if self.epoch > 25 and self.epoch % decay_freq == 0:\n","                self.optimizer.param_groups[0]['lr'] /= 1.05\n","        if '3node' in self.path:\n","            decay_freq = 10\n","            if self.epoch > 50 and self.epoch % decay_freq == 0:\n","                self.optimizer.param_groups[0]['lr'] /= 1.01\n","        if 'CA_solar' in self.path:\n","            decay_freq = 10\n","            if self.epoch > 50 and self.epoch % decay_freq == 0:\n","                self.optimizer.param_groups[0]['lr'] /= 1.01\n","        if 'traffic' in self.path:\n","            decay_freq = 10\n","            if self.epoch > 50 and self.epoch % decay_freq == 0:\n","                self.optimizer.param_groups[0]['lr'] /= 1.01\n","            else:\n","                print(\n","                    f'After Epoch {self.epoch} & Batch {batch}: \\n Test loss_g is {loss_g} \\n Test loss_c is {loss_c}.')\n","        return [loss_g_tot.item()/(batch+1), loss_c_tot.item()/(batch+1), 1 - numcorrect_tot.item() / Y.numel()]\n","\n","    def get_L_g(self, batch_idx, X, Y):\n","        # Return the L_g on X_train[batch_idx] by using the change-of-variable formula\n","        X_batch = X[batch_idx].flatten(start_dim=1)\n","        H_pred, log_det, transport_cost = self.model(\n","            X_batch, self.edge_index, self.edge_weight)\n","        # Reshape the tensor to N-by-n-by-C\n","        batch_size = len(batch_idx)\n","        H_pred = H_pred.reshape(batch_size, self.V, self.C)\n","        C_tmp = self.C\n","        if self.C > 2:\n","            # We need to evaluate likelihood in R^2, so reshape stuff.\n","            V_tmp = int(self.C/2)\n","            C_tmp = 2\n","            H_pred = H_pred.reshape(batch_size, V_tmp, C_tmp)\n","        tot_element = Y[batch_idx].numel()\n","        # We take sum so that the log-likelihood is average over graph, not each node.\n","        if '8_gaussian' in self.path:\n","            num_1 = (Y[batch_idx] == 1).sum().item()\n","            num_2 = (Y[batch_idx] == 2).sum().item()\n","            num_3 = (Y[batch_idx] == 3).sum().item()\n","        else:\n","            num_1 = (Y[batch_idx] == 1).sum().item()\n","            num_2, num_3 = 0, 0\n","        # Evaluate log like by class, where we assume at most 4 classes of Y exist\n","        if num_1 > 0:\n","            one_idx = (Y[batch_idx] == 1).unsqueeze(-1).repeat(1, 1, C_tmp)\n","            one_dim = int(one_idx.sum().item() / C_tmp)\n","            log_pH1 = self.base_dist1.log_prob(\n","                H_pred[one_idx].reshape(one_dim, C_tmp)).sum()\n","        else:\n","            log_pH1 = 0\n","        if num_2 > 0:\n","            two_idx = (Y[batch_idx] == 2).unsqueeze(-1).repeat(1, 1, C_tmp)\n","            two_dim = int(two_idx.sum().item() / C_tmp)\n","            log_pH2 = self.base_dist2.log_prob(\n","                H_pred[two_idx].reshape(two_dim, C_tmp)).sum()\n","        else:\n","            log_pH2 = 0\n","        if num_3 > 0:\n","            three_idx = (Y[batch_idx] == 3).unsqueeze(-1).repeat(1, 1, C_tmp)\n","            three_dim = int(three_idx.sum().item() / C_tmp)\n","            log_pH3 = self.base_dist3.log_prob(\n","                H_pred[three_idx].reshape(three_dim, C_tmp)).sum()\n","        else:\n","            log_pH3 = 0\n","        if tot_element - num_1-num_2-num_3 > 0:\n","            zero_idx = (Y[batch_idx] == 0).unsqueeze(-1).repeat(1, 1, C_tmp)\n","            zero_dim = int(zero_idx.sum().item() / C_tmp)\n","            log_pH0 = self.base_dist0.log_prob(\n","                H_pred[zero_idx].reshape(zero_dim, C_tmp)).sum()\n","        else:\n","            log_pH0 = 0\n","        log_pH = log_pH0 + log_pH1 + log_pH2 + log_pH3\n","        if self.C == 1 and ('one_Cheb' in self.path) or ('one_L3' in self.path):\n","            # Flow with ChebNet directly, so no need W2 regularization\n","            self.viz = True\n","            if 'W2' in self.path:\n","                self.viz = False\n","        if self.viz:\n","            # Need not transport cost\n","            logpx = (log_pH + log_det) / batch_size\n","        else:\n","            # For training, add transport cost\n","            logpx = (log_pH + log_det - self.gamma*transport_cost) / batch_size\n","        return -logpx\n","\n","    def get_L_c(self, batch_idx, X, Y):\n","        loss_f = torch.nn.BCEWithLogitsLoss(reduction='sum')\n","        X_batch = X[batch_idx].flatten(start_dim=1)\n","        H_pred = self.model.forward(\n","            X_batch, self.edge_index, self.edge_weight, logdet=False)\n","        batch_size = len(batch_idx)\n","        if self.C > 2:\n","            # NOTE: this is because FC treated graph example in R^V-x-C as a vector in \\R^V-by-C, so that we need reshaping for visualization\n","            V = int(self.C/2)\n","            H_pred = H_pred.reshape(batch_size, V, 2)\n","        else:\n","            H_pred = H_pred.reshape(batch_size, self.V, self.C)\n","        Y_pred = self.model.classification(H_pred)\n","        loss_c = self.mu * loss_f(Y_pred.flatten(),\n","                                  Y[batch_idx].flatten()) / batch_size\n","        Y_pred_round = torch.nn.Sigmoid()(Y_pred.flatten()).round()\n","        numcorrect = (Y_pred_round == Y[batch_idx].flatten()).sum()\n","        return [loss_c, numcorrect]\n","\n","    '''Train CGAN or CINN for comparison'''\n","\n","    def all_together_competitor(self):\n","        self.load_from_checkpoint_competitors()\n","        while self.epoch < self.epochs:\n","            if device.type == 'cuda':\n","                # Useful to avoid GPU allocation excess\n","                torch.cuda.empty_cache()\n","            # Visualize generation\n","            self.viz_generation_competitors()\n","            if self.Y_test is not None:\n","                self.viz_generation_competitors(viz_train=False)\n","            if 'CGAN' in self.path:\n","                if len(self.loss_GAN_train) > 10 and np.abs((self.loss_GAN_train[-1]-self.loss_GAN_train[-2])/self.loss_GAN_train[-2]) < self.stop_criterion:\n","                    # If consecutive dec. less than X%, then just break.\n","                    break\n","                loss_GAN_tot = self.batch_training_CGAN(train=True)\n","                self.loss_GAN_train.append(loss_GAN_tot)\n","                mem_report()  # Print GPU usage and availability\n","                if self.Y_test is not None:\n","                    with torch.no_grad():\n","                        loss_GAN_tot = self.batch_training_CGAN(train=False)\n","                        self.loss_GAN_test.append(loss_GAN_tot)\n","                else:\n","                    self.loss_GAN_test.append(0)\n","                print(\n","                    f'After Epoch {self.epoch}: CGAN train loss is {self.loss_GAN_train[-1]} \\n CGAN test loss is {self.loss_GAN_test[-1]}')\n","            if 'CINN' in self.path:\n","                if len(self.loss_CINN_train) > 10 and np.abs((self.loss_CINN_train[-1]-self.loss_CINN_train[-2])/self.loss_CINN_train[-2]) < self.stop_criterion:\n","                    # If consecutive dec. less than X%, then just break.\n","                    break\n","                loss_CINN_tot = self.batch_training_CINN(train=True)\n","                self.loss_CINN_train.append(loss_CINN_tot)\n","                mem_report()  # Print GPU usage and availability\n","                if self.Y_test is not None:\n","                    with torch.no_grad():\n","                        loss_CINN_tot = self.batch_training_CINN(train=False)\n","                        self.loss_CINN_test.append(loss_CINN_tot)\n","                else:\n","                    self.loss_CINN_test.append(0)\n","                print(\n","                    f'After Epoch {self.epoch}: CINN train loss is {self.loss_CINN_train[-1]} \\n CINN test loss is {self.loss_CINN_test[-1]}')\n","            self.viz_losses_competitors()\n","            self.save_checkpoint_competitors()\n","            self.epoch += 1\n","\n","    def batch_training_CGAN(self, train):\n","        if train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        loss_GAN_tot = 0\n","        V, C = self.V, self.C\n","        N_tmp = len(Y)\n","        batch_idxs = np.arange(N_tmp)\n","        for batch in range(int(np.ceil(N_tmp / self.batch_size))):\n","            # print(batch_idxs[-1])\n","            batch_idx = batch_idxs[batch\n","                                   * self.batch_size:np.min([(batch + 1) * self.batch_size, N_tmp])]\n","            num_b = len(batch_idx)\n","            X_batch = X[batch_idx]\n","            Y_batch = Y[batch_idx]\n","            nclasses = 4 if '8_gaussian' in self.path else 2\n","            Y_batch = F.one_hot(Y_batch.long(), num_classes=nclasses)\n","            torch.manual_seed(1103)\n","            noise_z = torch.randn(num_b, V, C).to(device)\n","            data_for_G = torch.cat((Y_batch, noise_z), -1).detach()\n","            X_hat_batch = self.net_G(\n","                data_for_G, self.edge_index, self.edge_weight)\n","            data_for_D_real = torch.cat((Y_batch, X_batch), -1).detach()\n","            data_for_D_fake = torch.cat((Y_batch, X_hat_batch), -1).detach()\n","            # First Discriminator loss\n","            self.net_D.set_requires_grad(True)\n","            pred_true = self.net_D(\n","                data_for_D_real, self.edge_index, self.edge_weight)\n","            pred_fake = self.net_D(\n","                data_for_D_fake, self.edge_index, self.edge_weight)\n","            # Original GAN loss\n","            # 0.5 is used to \"slow down\" rate of D learning relative to G\n","            # Here, D tries to maximize the likelihood of true data and minimize the likelihood of fake\n","            loss_D = -0.5*(torch.log(pred_true)\n","                           + torch.log(1-pred_fake)).mean()\n","            # # Wasserstain GAN loss\n","            # # Maximize the difference D(real)-D(fake)\n","            # loss_D = -0.5*(pred_true - pred_fake).mean()\n","            if train:\n","                self.optimizer_D.zero_grad()\n","                loss_D.backward()\n","                self.optimizer_D.step()\n","            # Then Generative loss\n","            self.net_D.set_requires_grad(False)\n","            data_for_D_fake = torch.cat((Y_batch, X_hat_batch), -1)\n","            pred_fake = self.net_D(\n","                data_for_D_fake, self.edge_index, self.edge_weight)\n","            # Original GAN loss\n","            # Here, G tries to maximize the likelihood of D thinking the prediction of G is \"real\"\n","            loss_G_1 = -torch.log(pred_fake).mean()\n","            # # Wasserstain GAN\n","            # loss_G_1 = -pred_fake.mean()\n","            lam = 1  # For l2 loss between truth and fake\n","            loss_G_2 = torch.linalg.norm(\n","                X_batch-X_hat_batch, ord=2, dim=(1, 2)).mean()\n","            # loss_G_2 = 0\n","            loss_G = loss_G_1 + lam*loss_G_2\n","            if train:\n","                self.optimizer_G.zero_grad()\n","                loss_G.backward()\n","                self.optimizer_G.step()\n","            loss_GAN_tot += (loss_D.item()+loss_G.item())*num_b\n","        return loss_GAN_tot/N_tmp\n","\n","    def batch_training_CINN(self, train):\n","        '''\n","            Code adopted from https://github.com/VLL-HD/analyzing_inverse_problems/tree/master/toy_8-modes of \"Analyzing Inverse Problems with Invertible Neural Networks\", ICLR 2019\n","\n","            Basically, it optimizers MMD in forward y, z direction and backward x direction, AS WELL AS the originally fitted errors in the padded y-z space and x space.\n","        '''\n","        if train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        loss_CINN_tot = 0\n","        V, C = self.V, self.C\n","        N_tmp = len(Y)\n","        batch_idxs = np.arange(N_tmp)\n","        for _ in ['hyper-parameters']:\n","            # For 8 Gaussian\n","            lambd_predict = 3.\n","            lambd_latent = 300.\n","            lambd_rev = 400.\n","            loss_factor = min(\n","                1., 2. * 0.002**(1. - (float(self.epoch) / self.epochs)))\n","            y_noise_scale = 1e-1\n","            zeros_noise_scale = 5e-2\n","        ndim_x, ndim_y, ndim_tot = self.ndim_x, self.ndim_y, self.ndim_tot\n","        ndim_z = ndim_x\n","        loss_backward = MMD_multiscale\n","        loss_latent = MMD_multiscale\n","        loss_fit = fit  # MSE\n","        C = self.C\n","        base_dist = MultivariateNormal(\n","            torch.zeros(C).to(device), torch.diag(torch.ones(C)).to(device))\n","        # If MMD on x-space is present from the start, the model can get stuck.\n","        # Instead, ramp it up exponetially.\n","        loss_factor = min(\n","            1., 2. * 0.002**(1. - (float(self.epoch) / self.epochs)))\n","        for batch in range(int(np.ceil(N_tmp / self.batch_size))):\n","            # print(batch_idxs[-1])\n","            batch_idx = batch_idxs[batch\n","                                   * self.batch_size:np.min([(batch + 1) * self.batch_size, N_tmp])]\n","            batch_size = len(batch_idx)\n","            x = X[batch_idx].flatten(start_dim=1)\n","            if self.V == 1:\n","                y = F.one_hot(Y[batch_idx].flatten().long(),\n","                              num_classes=4).float()\n","            else:\n","                y = F.one_hot(Y[batch_idx].long(),\n","                              num_classes=2).float().flatten(start_dim=1)\n","            # Pad X and Y to the high-dimension\n","            y_clean = y.clone()\n","            pad_x = zeros_noise_scale * torch.randn(batch_size, ndim_tot\n","                                                    - ndim_x, device=device)\n","            pad_yz = zeros_noise_scale * torch.randn(batch_size, ndim_tot\n","                                                     - ndim_y - ndim_z, device=device)\n","\n","            y += y_noise_scale * \\\n","                torch.randn(batch_size, ndim_y,\n","                            dtype=torch.float, device=device)\n","\n","            x, y = (torch.cat((x, pad_x),  dim=1),\n","                    torch.cat((torch.randn(batch_size, ndim_z, device=device), pad_yz, y),\n","                              dim=1))\n","            # Forward step:\n","            # output = model(x)\n","            output, log_det = self.model(x)\n","            loss_forward = 0\n","            if 'Nflow' in self.CINN_obj:\n","                log_prob = base_dist.log_prob(output[:, :ndim_z].reshape(\n","                    batch_size, self.V, C)).sum()\n","                loss_forward -= log_prob\n","                loss_forward -= log_det.sum()\n","                loss_forward /= batch_size\n","            else:\n","                # MMD-based\n","                # Shorten output, and remove gradients wrt y, for latent loss\n","                y_short = torch.cat((y[:, :ndim_z], y[:, -ndim_y:]), dim=1)\n","                loss_y = lambd_predict * \\\n","                    loss_fit(output[:, ndim_z:], y[:, ndim_z:])\n","                loss_forward += loss_y\n","                output_block_grad = torch.cat((output[:, :ndim_z],\n","                                               output[:, -ndim_y:].data), dim=1)\n","\n","                loss_z = lambd_latent * loss_latent(output_block_grad, y_short)\n","                loss_forward += loss_z\n","                # Backward step:\n","                pad_yz = zeros_noise_scale * torch.randn(batch_size, ndim_tot\n","                                                         - ndim_y - ndim_z, device=device)\n","                y = y_clean + y_noise_scale * \\\n","                    torch.randn(batch_size, ndim_y, device=device)\n","\n","                orig_z_perturbed = (output.data[:, :ndim_z] + y_noise_scale\n","                                    * torch.randn(batch_size, ndim_z, device=device))\n","                y_rev = torch.cat((orig_z_perturbed, pad_yz,\n","                                   y), dim=1)\n","                y_rev_rand = torch.cat((torch.randn(batch_size, ndim_z, device=device), pad_yz,\n","                                        y), dim=1)\n","\n","                # output_rev = model(y_rev, rev=True)\n","                # output_rev_rand = model(y_rev_rand, rev=True)\n","                # Chen: their code errors\n","                output_rev = self.model(y_rev, rev=True)[0]\n","                output_rev_rand = self.model(y_rev_rand, rev=True)[0]\n","                l_rev = 0\n","                loss_x_1 = (\n","                    lambd_rev\n","                    * loss_factor\n","                    * loss_backward(output_rev_rand[:, :ndim_x],\n","                                    x[:, :ndim_x])\n","                )\n","                l_rev += loss_x_1\n","                loss_x_2 = lambd_predict * loss_fit(output_rev, x)\n","                l_rev += loss_x_2\n","                loss_CINN_tot += l_rev.data.item()*batch_size\n","            loss_CINN_tot += loss_forward.data.item()*batch_size\n","            for param in self.model.parameters():\n","                # Chen, they did not include this earlier\n","                if param.grad is None:\n","                    continue\n","                param.grad.data.clamp_(-15.00, 15.00)\n","            if train:\n","                self.optimizer.zero_grad()\n","                loss_forward.backward()\n","                if 'Nflow' not in self.CINN_obj:\n","                    l_rev.backward()\n","                self.optimizer.step()\n","        return loss_CINN_tot/N_tmp\n","\n","    '''Visulization, loading, and saving for competitors '''\n","\n","    def viz_losses_competitors(self):\n","        plt.rcParams['axes.titlesize'] = 18\n","        plt.rcParams['font.size'] = 18\n","        plt.rcParams['figure.titlesize'] = 22\n","        plt.rcParams['legend.fontsize'] = 18\n","        if 'CGAN' in self.path:\n","            loss_train, loss_test = self.loss_GAN_train, self.loss_GAN_test\n","            title = 'CGAN loss'\n","        if 'CINN' in self.path:\n","            loss_train, loss_test = self.loss_CINN_train, self.loss_CINN_test\n","            title = 'CINN loss'\n","        if np.mod(self.epoch + 1, 5) or self.final_viz:\n","            fig, ax = plt.subplots(figsize=(4, 4))\n","            ax.plot(loss_train, label=r'Training', color='black')\n","            ax.plot(loss_test, label=r'Test', color='blue')\n","            ax.set_title(title)\n","            ax.legend()\n","            save_prefix = self.path+'/' if self.prefix == '' else self.prefix\n","            fig.savefig(f'{save_prefix}Losses_epoch{self.epoch+1}.png',\n","                        dpi=150, bbox_inches='tight', pad_inches=0)\n","            plt.show()\n","            plt.close()\n","\n","    def viz_generation_competitors(self, viz_train=True):\n","        if viz_train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        V, C = self.V, self.C\n","        Unique_Y, counts_Y = torch.unique(Y, return_counts=True, dim=0)\n","        counts_Y, idx = torch.sort(counts_Y, descending=True)\n","        Unique_Y = Unique_Y[idx]\n","        viz_freq = 5 if self.num_viz == 1 else 15\n","        if self.epoch > 50:\n","            viz_freq = 30\n","        # if 'CGAN' in self.path:\n","        #     viz_freq = 50\n","        if self.final_viz:\n","            self.two_sample_stat = {}\n","        if np.mod(self.epoch + 1, viz_freq) == 0 or self.final_viz:\n","            for pp, Y_row in enumerate(Unique_Y[:self.num_viz]):\n","                if self.V == 1 and self.C == 2:\n","                    # Two moon plot\n","                    Y_row = None\n","                which_rows = (Y == Y_row).all(\n","                    dim=1) if Y_row is not None else torch.tensor([True]).repeat(X.shape[0])\n","                num_to_gen = sum(which_rows).item()\n","                print(\n","                    f'Checking generation at \\n {Y_row} with {num_to_gen} out of {len(Y)} data')\n","                X_sub, Y_sub = X[which_rows], Y[which_rows]\n","                if 'CGAN' in self.path:\n","                    torch.manual_seed(1103)\n","                    noise_z = torch.randn(num_to_gen, V, C).to(device)\n","                    nclasses = 4 if '8_gaussian' in self.path else 2\n","                    Y_sub = F.one_hot(Y_sub.long(), num_classes=nclasses)\n","                    with torch.no_grad():\n","                        X_pred = self.net_G(torch.cat((Y_sub, noise_z), -1),\n","                                            self.edge_index, self.edge_weight).cpu()\n","                    H, H_pred = None, None\n","                if 'CINN' in self.path:\n","                    y_noise_scale = 1e-1\n","                    zeros_noise_scale = 5e-2\n","                    ndim_x, ndim_y, ndim_tot = self.ndim_x, self.ndim_y, self.ndim_tot\n","                    ndim_z = ndim_x\n","                    x_samps = X_sub.flatten(start_dim=1)\n","                    x_pad = torch.zeros(\n","                        num_to_gen, ndim_tot - ndim_x).to(device)\n","                    x_samps = torch.cat((x_samps, x_pad), dim=1).to(device)\n","                    if self.V == 1:\n","                        y_samps = F.one_hot(\n","                            Y_sub.flatten().long(), num_classes=4).float()\n","                    else:\n","                        y_samps = F.one_hot(\n","                            Y_sub.long(), num_classes=2).float().flatten(start_dim=1)\n","                    y_samps += y_noise_scale * \\\n","                        torch.randn(num_to_gen, ndim_y).to(device)\n","                    y_samps = torch.cat([torch.randn(num_to_gen, ndim_z).to(device),\n","                                         zeros_noise_scale\n","                                         * torch.zeros(num_to_gen,\n","                                                       ndim_tot - ndim_y - ndim_z).to(device),\n","                                         y_samps], dim=1)\n","                    y_samps = y_samps.to(device)\n","                    with torch.no_grad():\n","                        # # Using hand-coded R-NVP, which suffers from non-invertibility issue\n","                        # X_pred = self.model.inverse(noise_z, Y_sub).cpu()\n","                        # H_pred = self.model(X_sub.flatten(start_dim=1), Y_sub)[0].cpu()\n","                        # Using the GLOW model\n","                        X_pred = self.model(y_samps, rev=True)[\n","                            0][:, :ndim_x].cpu()\n","                        H_pred = self.model(x_samps)[0][:, :ndim_z].cpu()\n","                    if C > 1:\n","                        X_pred = X_pred.reshape(num_to_gen, V, C)\n","                    # H = noise_z.reshape(num_to_gen, V, C).cpu()\n","                    # H_pred = H_pred.reshape(num_to_gen, V, C)\n","                    H, H_pred = None, None  # So figures are more compact\n","                viz.plt_generation_fig_competitor(\n","                    self, X_sub, X_pred, Y[which_rows], H, H_pred)\n","                train_test_save = '_train' if viz_train else '_test'\n","                save_prefix = self.path+'/' if self.prefix == '' else self.prefix\n","                self.fig_gen.savefig(f'{save_prefix}Generation{train_test_save}_epoch{self.epoch+1}_top{pp+1}_occurrences.png',\n","                                     dpi=150, bbox_inches='tight', pad_inches=0)\n","                # Also report quantitative metrics:\n","                if self.final_viz:\n","                    # Record num of obs.\n","                    X_sub, X_pred = X_sub.flatten(\n","                        start_dim=1), X_pred.flatten(start_dim=1)\n","                    self.two_sample_stat[Y_row] = [num_to_gen]\n","                    for method in ['MMD', 'Energy']:\n","                        if method == 'MMD':\n","                            for alphas in [[0.1], [1.0], [5.0], [10.0]]:\n","                                ret = two_sample_mtd(\n","                                    X_sub, X_pred, alphas=alphas, method=method)\n","                                self.two_sample_stat[Y_row].append(ret)\n","                        else:\n","                            ret = two_sample_mtd(X_sub, X_pred, method=method)\n","                            self.two_sample_stat[Y_row].append(ret)\n","\n","    def load_from_checkpoint_competitors(self):\n","        if len(self.model) > 1:\n","            # CGAN\n","            self.path = f'{self.data_name}_CGAN'\n","            self.net_D, self.net_G = self.model\n","            self.optimizer_D, self.optimizer_G = self.optimizer\n","            isExist = os.path.exists(self.path)\n","            if not isExist:\n","                # Create a new directory because it does not exist\n","                os.makedirs(self.path)\n","                print(\"The new directory is created!\")\n","            self.checkpoint_savename = f'{self.path}/CGAN_checkpoint'\n","            if os.path.exists(self.checkpoint_savename) and self.resume_checkpoint:\n","                # Resume training if this file exist\n","                checkpoint = torch.load(self.checkpoint_savename, map_location=torch.device(\n","                    'cpu')) if self.cpu_load else torch.load(self.checkpoint_savename)\n","                self.net_D.load_state_dict(checkpoint['D_state_dict'])\n","                self.optimizer_D.load_state_dict(checkpoint['D_optimizer'])\n","                self.net_G.load_state_dict(checkpoint['G_state_dict'])\n","                self.optimizer_G.load_state_dict(checkpoint['G_optimizer'])\n","                self.epoch = checkpoint['epoch']\n","                self.loss_GAN_train, self.loss_GAN_test = checkpoint['loss_ls']\n","            else:\n","                self.epoch = 0\n","                self.loss_GAN_train, self.loss_GAN_test = [], []\n","        else:\n","            # CINN\n","            self.model = self.model[0]\n","            self.path = f'{self.data_name}_CINN'\n","            isExist = os.path.exists(self.path)\n","            if not isExist:\n","                # Create a new directory because it does not exist\n","                os.makedirs(self.path)\n","                print(\"The new directory is created!\")\n","            self.checkpoint_savename = f'{self.path}/CINN_checkpoint'\n","            if os.path.exists(self.checkpoint_savename) and self.resume_checkpoint:\n","                # Resume training if this file exist\n","                checkpoint = torch.load(self.checkpoint_savename, map_location=torch.device(\n","                    'cpu')) if self.cpu_load else torch.load(self.checkpoint_savename)\n","                self.model.load_state_dict(checkpoint['state_dict'])\n","                self.optimizer.load_state_dict(checkpoint['optimizer'])\n","                self.epoch = checkpoint['epoch']\n","                self.loss_CINN_train, self.loss_CINN_test = checkpoint['loss_ls']\n","            else:\n","                self.epoch = 0\n","                self.loss_CINN_train, self.loss_CINN_test = [], []\n","\n","    def save_checkpoint_competitors(self):\n","        if 'CGAN' in self.path:\n","            checkpoint = {'epoch': self.epoch + 1, 'loss_ls': [self.loss_GAN_train, self.loss_GAN_test],\n","                          'D_state_dict': self.net_D.state_dict(), 'D_optimizer': self.optimizer_D.state_dict(),\n","                          'G_state_dict': self.net_G.state_dict(), 'G_optimizer': self.optimizer_G.state_dict()}\n","        if 'CINN' in self.path:\n","            checkpoint = {'epoch': self.epoch + 1, 'loss_ls': [self.loss_CINN_train, self.loss_CINN_test],\n","                          'state_dict': self.model.state_dict(), 'optimizer': self.optimizer.state_dict()}\n","        torch.save(checkpoint, self.checkpoint_savename)\n","\n","    '''Visualizations of our method'''\n","\n","    def viz_generation(self, viz_train=True):\n","        if viz_train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        Unique_Y, counts_Y = torch.unique(Y, return_counts=True, dim=0)\n","        counts_Y, idx = torch.sort(counts_Y, descending=True)\n","        Unique_Y = Unique_Y[idx]\n","        # In real data or the \"artificial\" examples\n","        self.get_reindex(viz_train)\n","        viz_freq = 5 if self.num_viz == 1 else 15\n","        if self.epoch > 50:\n","            viz_freq = 30\n","        # if self.epoch == 0 or np.mod(self.epoch + 1, viz_freq) == 0 or self.final_viz:\n","        if self.final_viz:\n","            self.two_sample_stat = {}\n","        if np.mod(self.epoch + 1, viz_freq) == 0 or self.final_viz:\n","            for pp, Y_row in enumerate(Unique_Y[:self.num_viz]):\n","                if self.V == 1 and self.C == 2:\n","                    # Two moon plot\n","                    Y_row = None\n","                print(\n","                    f'Checking generation at \\n {Y_row} with {counts_Y[pp]} out of {len(Y)} data')\n","                torch.manual_seed(1103)\n","                # pdb.set_trace()\n","                if '8_gaussian' in self.path:\n","                    H0 = self.base_dist0.rsample(sample_shape=(self.N0,))\n","                    H1 = self.base_dist1.rsample(sample_shape=(self.N1,))\n","                    H2 = self.base_dist2.rsample(sample_shape=(self.N2,))\n","                    H3 = self.base_dist3.rsample(sample_shape=(self.N3,))\n","                    H_full = torch.vstack([H0, H1, H2, H3]).to(device)[\n","                        self.re_index]\n","                else:\n","                    H0 = self.base_dist0.rsample(sample_shape=(self.N0,))\n","                    H1 = self.base_dist1.rsample(sample_shape=(self.N1,))\n","                    H_full = torch.vstack([H0, H1]).to(device)[self.re_index]\n","                H_full = H_full.reshape(X.shape)\n","                if self.final_viz:\n","                    self.two_sample_mtd = two_sample_mtd\n","                # # Visualize results\n","                viz.visualize_generation_one_graph(\n","                    self, X, Y, H_full, Y_row)\n","                train_test_save = '_train' if viz_train else '_test'\n","                save_prefix = self.path+'/' if self.prefix == '' else self.prefix\n","                self.fig_gen.savefig(f'{save_prefix}Generation{train_test_save}_epoch{self.epoch+1}_top{pp+1}_occurrences.png',\n","                                     dpi=150, bbox_inches='tight', pad_inches=0)\n","                if self.C == 1:\n","                    # Also visualize correlation matrix, so we save it here as well\n","                    save_prefix = self.path+'/' if self.prefix == '' else self.prefix\n","                    self.fig_corr.savefig(\n","                        f'{save_prefix}correlation_matrices_epoch{self.epoch+1}.png', dpi=150, bbox_inches='tight', pad_inches=0)\n","                if self.V == 1 and self.C == 2:\n","                    # No lonegr need to plot, because we have finished two-moon plot\n","                    break\n","\n","    def get_GIF(self, Y_row, viz_train=True, from_X_to_H=False):\n","        self.from_X_to_H = from_X_to_H\n","        if self.C == 1:\n","            raise ValueError('GIF for C=1 not yet considered')\n","        if viz_train:\n","            X, Y = self.X_train, self.Y_train\n","        else:\n","            X, Y = self.X_test, self.Y_test\n","        which_rows = (Y == Y_row).all(\n","            dim=1) if Y_row is not None else torch.tensor([True]).repeat(X.shape[0])\n","        self.get_reindex(viz_train)\n","        torch.manual_seed(1103)\n","        H0 = self.base_dist0.rsample(sample_shape=(self.N0,))\n","        H1 = self.base_dist1.rsample(sample_shape=(self.N1,))\n","        H_full = torch.vstack([H0, H1]).to(device)[self.re_index]\n","        H_full = H_full.reshape(X.shape)\n","        X, Y, H_full = X[which_rows], Y[which_rows], H_full[which_rows]\n","        num_plot = min(400, X.shape[0])\n","        viz.save_trajectory_revised(\n","            self, X[:num_plot], Y[:num_plot], H_full[:num_plot])\n","\n","    def viz_losses(self):\n","        if np.mod(self.epoch + 1, 5) == 0 or self.final_viz:\n","            fig = viz.losses_and_error_plt_real_data_on_graph(\n","                self.loss_g_ls_train, self.loss_g_ls_test, self.loss_c_ls_train, self.loss_c_ls_test, self.classify_error_ls_train, self.classify_error_ls_test)\n","            save_prefix = self.path+'/' if self.prefix == '' else self.prefix\n","            fig.savefig(f'{save_prefix}Losses_epoch{self.epoch+1}.png',\n","                        dpi=150, bbox_inches='tight', pad_inches=0)\n","\n","    '''Saving and/or loading from checkpoints for our method'''\n","\n","    def save_checkpoint(self):\n","        # Save checkpoint at the current epoch, so we can resume training etc\n","        self.loss_ls = [self.loss_g_ls_train, self.loss_g_ls_test, self.loss_c_ls_train,\n","                        self.loss_c_ls_test, self.classify_error_ls_train, self.classify_error_ls_test]\n","        checkpoint = {'epoch': self.epoch + 1, 'loss_ls': self.loss_ls,\n","                      'state_dict': self.model.state_dict(), 'optimizer': self.optimizer.state_dict(),\n","                      'gen_net': self.gen_net.state_dict()}\n","        if len(self.non_invertible_ls) > 0 and self.non_invertible_ls[-1] == self.epoch:\n","            # Rename the latest saved model, as its NEXT epoch (which is the current epoch) has non-invertibility issue\n","            # torch.save(\n","            #     checkpoint, f'{self.checkpoint_savename}_non_invertible_at_epoch{self.epoch}')\n","            # os.rename(self.checkpoint_savename, f'{self.checkpoint_savename}_non_invertible_next_epoch{self.epoch}')\n","            shutil.copy(self.checkpoint_savename,\n","                        f'{self.checkpoint_savename}_non_invertible_next_epoch{self.epoch}')\n","        else:\n","            torch.save(checkpoint, self.checkpoint_savename)\n","\n","    def load_from_checkpoint(self):\n","        # Check whether the specified folder exists or not\n","        self.path = self.data_name\n","        isExist = os.path.exists(self.path)\n","        if not isExist:\n","            # Create a new directory because it does not exist\n","            os.makedirs(self.path)\n","            print(\"The new directory is created!\")\n","        # Load from a previous training iteration\n","        self.checkpoint_savename = f'{self.path}/IResNet_checkpoint_dim_{self.dim}_nblocks_{self.nblocks}'\n","        if os.path.exists(self.checkpoint_savename) and self.resume_checkpoint:\n","            # Resume training if this file exist\n","            checkpoint = torch.load(self.checkpoint_savename, map_location=torch.device(\n","                'cpu')) if self.cpu_load else torch.load(self.checkpoint_savename)\n","            self.model.load_state_dict(checkpoint['state_dict'])\n","            self.gen_net.load_state_dict(checkpoint['gen_net'])\n","            self.optimizer.load_state_dict(checkpoint['optimizer'])\n","            self.epoch = checkpoint['epoch']\n","            self.loss_g_ls_train, self.loss_g_ls_test, self.loss_c_ls_train, self.loss_c_ls_test, self.classify_error_ls_train, self.classify_error_ls_test = checkpoint[\n","                'loss_ls']\n","        else:\n","            self.epoch = 0\n","            self.loss_g_ls_train, self.loss_g_ls_test, self.loss_c_ls_train, self.loss_c_ls_test, self.classify_error_ls_train, self.classify_error_ls_test = [], [], [], [], [], []\n","\n","    '''Other necessary helpers for our method'''\n","\n","    def get_H_cond_Y(self):\n","        if '8_gaussian' in self.path:\n","            base_mu0 = self.gen_net(torch.Tensor(\n","                [1, 0, 0, 0]).to(device)).to(device)\n","            base_mu1 = self.gen_net(torch.Tensor(\n","                [0, 1, 0, 0]).to(device)).to(device)\n","            base_mu2 = self.gen_net(torch.Tensor(\n","                [0, 0, 1, 0]).to(device)).to(device)\n","            base_mu3 = self.gen_net(torch.Tensor(\n","                [0, 0, 0, 1]).to(device)).to(device)\n","            dist = 0.1\n","            base_cov = (torch.eye(self.gen_net.fc.out_features)\n","                        * dist).to(device)\n","            self.base_dist0 = MultivariateNormal(base_mu0, base_cov)\n","            self.base_dist1 = MultivariateNormal(base_mu1, base_cov)\n","            self.base_dist2 = MultivariateNormal(base_mu2, base_cov)\n","            self.base_dist3 = MultivariateNormal(base_mu3, base_cov)\n","\n","        else:\n","            base_mu0 = self.gen_net(torch.Tensor([1, 0]).to(device)).to(device)\n","            base_mu1 = self.gen_net(torch.Tensor([0, 1]).to(device)).to(device)\n","            dist = 0.1 if self.X_test is None else torch.linalg.norm(\n","                base_mu0 - base_mu1).cpu() / 8  # First for 3 node simulation, Second for real-data\n","            base_cov = (torch.eye(self.gen_net.fc.out_features)\n","                        * dist).to(device)\n","            if self.C > 1:\n","                self.base_dist0 = MultivariateNormal(base_mu0, base_cov)\n","                self.base_dist1 = MultivariateNormal(base_mu1, base_cov)\n","            else:\n","                base_cov = torch.ones(1).to(device)\n","                self.base_dist0 = Normal(base_mu0, base_cov)\n","                self.base_dist1 = Normal(base_mu1, base_cov)\n","\n","    def get_reindex(self, viz_train=True):\n","        # Generating H0 and H1 conditionally AND obey the order in graph label\n","        suffix = '_train' if viz_train else '_test'\n","        idx_name = f'{self.path}/get_idx{suffix}'\n","        if os.path.exists(idx_name):\n","            idx_dict = torch.load(idx_name)\n","            if '8_gaussian' in self.path:\n","                self.re_index, self.N0, self.N1, self.N2, self.N3 = idx_dict['indices']\n","            else:\n","                self.re_index, self.N0, self.N1 = idx_dict['indices']\n","        else:\n","            idx_dict = {}\n","            Y = self.Y_train if viz_train else self.Y_test\n","            if '8_gaussian' in self.path:\n","                N0 = (Y == 0).sum().item()\n","                N1 = (Y == 1).sum().item()\n","                N2 = (Y == 2).sum().item()\n","                N3 = (Y == 3).sum().item()\n","                raw_index = [0] * N0 + [1] * N1 + [2] * N2 + [3] * N3\n","            else:\n","                N1 = int(Y.sum().item())\n","                N0 = int(Y.numel() - N1)\n","                raw_index = [0] * N0 + [1] * N1\n","            old_index = Y.flatten().tolist()\n","            # Below can take some time for large graph and sample size\n","            # So instead, I only re-index the first num graphs.\n","            re_index = []\n","            for val in old_index:\n","                idx = raw_index.index(val)\n","                raw_index[idx] = 100000\n","                re_index.append(idx)\n","            if '8_gaussian' in self.path:\n","                self.re_index, self.N0, self.N1, self.N2, self.N3 = re_index, N0, N1, N2, N3\n","                idx_dict['indices'] = [re_index, N0, N1, N2, N3]\n","            else:\n","                self.re_index, self.N0, self.N1 = re_index, N0, N1\n","                idx_dict['indices'] = [re_index, N0, N1]\n","            if self.save_reindex:\n","                torch.save(idx_dict, idx_name)\n","\n","    def check_model_inversion(self):\n","        # On-dist inversion error\n","        N_sub = 10\n","        # Randomly sample some training indices\n","        with torch.no_grad():\n","            X_rand = self.X_train[np.random.choice(np.arange(self.N), N_sub)]\n","            X_for, log_det, _ = self.model(\n","                X_rand.flatten(start_dim=1), self.edge_index, self.edge_weight)\n","            X_for = X_for.reshape(N_sub, self.V, self.C).to(device)\n","            X_hat = self.model.inverse(\n","                X_for, self.edge_index, self.edge_weight)\n","            err_percent_on_dist = torch.linalg.norm(\n","                X_hat-X_rand)/torch.linalg.norm(X_rand)\n","            if self.X_test is not None:\n","                X_rand = self.X_test[np.random.choice(\n","                    np.arange(self.X_test.shape[0]), N_sub)]\n","                X_for, log_det, _ = self.model(\n","                    X_rand.flatten(start_dim=1), self.edge_index, self.edge_weight)\n","                X_for = X_for.reshape(N_sub, self.V, self.C).to(device)\n","                X_hat = self.model.inverse(\n","                    X_for, self.edge_index, self.edge_weight)\n","                err_percent_on_dist_test = torch.linalg.norm(\n","                    X_hat-X_rand)/torch.linalg.norm(X_rand)\n","            # Off-dist. inversion error\n","            X = torch.rand(N_sub, self.V, self.C).to(device)\n","            X_for, log_det, _ = self.model(\n","                X.flatten(start_dim=1), self.edge_index, self.edge_weight)\n","            X_for = X_for.reshape(N_sub, self.V, self.C).to(device)\n","            X_hat = self.model.inverse(\n","                X_for, self.edge_index, self.edge_weight)\n","            err_percent_off_dist = torch.linalg.norm(\n","                X_hat-X)/torch.linalg.norm(X)\n","        if err_percent_off_dist < 1e-4 and err_percent_on_dist < 1e-4:\n","            print(\n","                f'Model is invertible \\n On Dist. Inversion Error is {err_percent_on_dist*100}% \\n Off Dist. Inversion Error is {err_percent_off_dist*100}%')\n","            if self.X_test is not None:\n","                print(\n","                    f'On Dist. Inversion Error on X_test is {err_percent_on_dist_test*100}%')\n","            print(f'Past non-invertible epochs are {self.non_invertible_ls}')\n","        else:\n","            print(\n","                f'Model non-invertible \\n On Dist. Inversion Error is {err_percent_on_dist*100}% \\n Off Dist. Inversion Error is {err_percent_off_dist*100}%')\n","            if self.X_test is not None:\n","                print(\n","                    f'On Dist. Inversion Error on X_test is {err_percent_on_dist_test*100}%')\n","            self.non_invertible_ls.append(self.epoch)\n","            self.save_checkpoint()\n","            print(f'Weight Reduction in place')\n","            self.model.small_weights()\n","\n","# For CINN, MMD\n","\n","\n","def MMD_multiscale(x, y):\n","    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n","\n","    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n","    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n","\n","    dxx = rx.t() + rx - 2.*xx\n","    dyy = ry.t() + ry - 2.*yy\n","    dxy = rx.t() + ry - 2.*zz\n","\n","    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n","                  torch.zeros(xx.shape).to(device),\n","                  torch.zeros(xx.shape).to(device))\n","\n","    for a in [0.05, 0.2, 0.9]:\n","        XX += a**2 * (a**2 + dxx)**-1\n","        YY += a**2 * (a**2 + dyy)**-1\n","        XY += a**2 * (a**2 + dxy)**-1\n","\n","    return torch.mean(XX + YY - 2.*XY)\n","\n","\n","def fit(input, target):\n","    return torch.mean((input - target)**2)\n","\n","\n","def mem_report():\n","    if device.type == 'cuda':\n","        GPUs = GPUtil.getGPUs()\n","        for i, gpu in enumerate(GPUs):\n","            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n","                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n","    else:\n","        print(\"CPU RAM Free: \"\n","              + humanize.naturalsize(psutil.virtual_memory().available))\n","\n","# Two-sample tests:\n","\n","\n","def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n","    r\"\"\"Compute the matrix of all squared pairwise distances.\n","    Arguments\n","    ---------\n","    sample_1 : torch.Tensor or Variable\n","        The first sample, should be of shape ``(n_1, d)``.\n","    sample_2 : torch.Tensor or Variable\n","        The second sample, should be of shape ``(n_2, d)``.\n","    norm : float\n","        The l_p norm to be used.\n","    Returns\n","    -------\n","    torch.Tensor or Variable\n","        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to\n","        ``|| sample_1[i, :] - sample_2[j, :] ||_p``.\"\"\"\n","    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n","    norm = float(norm)\n","    if norm == 2.:\n","        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n","        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n","        norms = (norms_1.expand(n_1, n_2)\n","                 + norms_2.transpose(0, 1).expand(n_1, n_2))\n","        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n","        return torch.sqrt(eps + torch.abs(distances_squared))\n","    else:\n","        dim = sample_1.size(1)\n","        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n","        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n","        differences = torch.abs(expanded_1 - expanded_2) ** norm\n","        inner = torch.sum(differences, dim=2, keepdim=False)\n","        return (eps + inner) ** (1. / norm)\n","\n","\n","class MMDStatistic:\n","    r\"\"\"The *unbiased* MMD test of :cite:`gretton2012kernel`.\n","    The kernel used is equal to:\n","    .. math ::\n","        k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j\\|x - x'\\|^2},\n","    for the :math:`\\alpha_j` proved in :py:meth:`~.MMDStatistic.__call__`.\n","    Arguments\n","    ---------\n","    n_1: int\n","        The number of points in the first sample.\n","    n_2: int\n","        The number of points in the second sample.\"\"\"\n","\n","    def __init__(self, n_1, n_2):\n","        self.n_1 = n_1\n","        self.n_2 = n_2\n","\n","        # The three constants used in the test.\n","        self.a00 = 1. / (n_1 * (n_1 - 1))\n","        self.a11 = 1. / (n_2 * (n_2 - 1))\n","        self.a01 = - 1. / (n_1 * n_2)\n","\n","    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n","        r\"\"\"Evaluate the statistic.\n","        The kernel used is\n","        .. math::\n","            k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j \\|x - x'\\|^2},\n","        for the provided ``alphas``.\n","        Arguments\n","        ---------\n","        sample_1: :class:`torch:torch.autograd.Variable`\n","            The first sample, of size ``(n_1, d)``.\n","        sample_2: variable of shape (n_2, d)\n","            The second sample, of size ``(n_2, d)``.\n","        alphas : list of :class:`float`\n","            The kernel parameters.\n","        ret_matrix: bool\n","            If set, the call with also return a second variable.\n","            This variable can be then used to compute a p-value using\n","            :py:meth:`~.MMDStatistic.pval`.\n","        Returns\n","        -------\n","        :class:`float`\n","            The test statistic.\n","        :class:`torch:torch.autograd.Variable`\n","            Returned only if ``ret_matrix`` was set to true.\"\"\"\n","        sample_12 = torch.cat((sample_1, sample_2), 0)\n","        distances = pdist(sample_12, sample_12, norm=2)\n","\n","        kernels = None\n","        for alpha in alphas:\n","            kernels_a = torch.exp(- alpha * distances ** 2)\n","            if kernels is None:\n","                kernels = kernels_a\n","            else:\n","                kernels = kernels + kernels_a\n","\n","        k_1 = kernels[:self.n_1, :self.n_1]\n","        k_2 = kernels[self.n_1:, self.n_1:]\n","        k_12 = kernels[:self.n_1, self.n_1:]\n","\n","        mmd = (2 * self.a01 * k_12.sum()\n","               + self.a00 * (k_1.sum() - torch.trace(k_1))\n","               + self.a11 * (k_2.sum() - torch.trace(k_2)))\n","        if ret_matrix:\n","            return mmd, kernels\n","        else:\n","            return mmd\n","\n","\n","class EnergyStatistic:\n","    r\"\"\"The energy test of :cite:`szekely2013energy`.\n","\n","    Arguments\n","    ---------\n","    n_1: int\n","        The number of points in the first sample.\n","    n_2: int\n","        The number of points in the second sample.\"\"\"\n","\n","    def __init__(self, n_1, n_2):\n","        self.n_1 = n_1\n","        self.n_2 = n_2\n","\n","        self.a00 = - 1. / (n_1 * n_1)\n","        self.a11 = - 1. / (n_2 * n_2)\n","        self.a01 = 1. / (n_1 * n_2)\n","\n","    def __call__(self, sample_1, sample_2, ret_matrix=False):\n","        r\"\"\"Evaluate the statistic.\n","\n","        Arguments\n","        ---------\n","        sample_1: :class:`torch:torch.autograd.Variable`\n","            The first sample, of size ``(n_1, d)``.\n","        sample_2: variable of shape (n_2, d)\n","            The second sample, of size ``(n_2, d)``.\n","        norm : float\n","            Which norm to use when computing distances.\n","        ret_matrix: bool\n","            If set, the call with also return a second variable.\n","\n","            This variable can be then used to compute a p-value using\n","            :py:meth:`~.EnergyStatistic.pval`.\n","\n","        Returns\n","        -------\n","        :class:`float`\n","            The test statistic.\n","        :class:`torch:torch.autograd.Variable`\n","            Returned only if ``ret_matrix`` was set to true.\"\"\"\n","        sample_12 = torch.cat((sample_1, sample_2), 0)\n","        distances = pdist(sample_12, sample_12, norm=2)\n","        d_1 = distances[:self.n_1, :self.n_1].sum()\n","        d_2 = distances[-self.n_2:, -self.n_2:].sum()\n","        d_12 = distances[:self.n_1, -self.n_2:].sum()\n","\n","        loss = 2 * self.a01 * d_12 + self.a00 * d_1 + self.a11 * d_2\n","\n","        if ret_matrix:\n","            return loss, distances\n","        else:\n","            return loss\n","\n","\n","def two_sample_mtd(x, y, alphas=[1.0], method='MMD'):\n","    \"\"\"\n","        Return the statistics based on input method\n","    \"\"\"\n","    if x.device.type != 'cpu':\n","        x, y = x.cpu().detach(), y.cpu().detach()\n","    torch.manual_seed(1103)\n","    indexes = torch.randperm(x.shape[0])\n","    torch.manual_seed(1111)\n","    indexes1 = torch.randperm(y.shape[0])\n","    x, y = x[indexes], y[indexes1]\n","    cuda = True if device.type != 'cpu' else False\n","    N_1, N_2 = x.shape[0], y.shape[0]\n","    if method == 'MMD':\n","        mtd = MMDStatistic(N_1, N_2)\n","        return mtd(x, y, alphas).item()\n","    if method == 'Energy':\n","        mtd = EnergyStatistic(N_1, N_2)\n","        return mtd(x, y).item()\n","\n","# Other helpers\n","\n","\n","def get_stat_frame_from_dict(res_dict):\n","    '''\n","        res_dict: {generative method: {unique_Y: [counts, two_sample_stats]}}\n","    '''\n","    keys_gen_mtd = list(res_dict.keys())\n","    keys_uniq_Y = list(res_dict[keys_gen_mtd[0]].keys())\n","    nrow, ncol = len(keys_gen_mtd), len(\n","        res_dict[keys_gen_mtd[0]][keys_uniq_Y[0]])-1  # Num generative method & Num two-sample tests\n","    l_Y = len(keys_uniq_Y)\n","    ncol_full = ncol*l_Y\n","    res_array = np.zeros((nrow, ncol))\n","    res_array_full = np.zeros((nrow, ncol_full))\n","    counts = np.array([res_dict[keys_gen_mtd[0]][key][0]\n","                      for key in keys_uniq_Y])\n","    weights = counts/counts.sum()\n","    for i in range(nrow):\n","        keys_uniq_Y = list(res_dict[keys_gen_mtd[i]].keys())\n","        for j in range(ncol):\n","            vals = np.array([res_dict[keys_gen_mtd[i]][key][j+1]\n","                            for key in keys_uniq_Y])\n","            res_array[i, j] = weights.dot(vals)\n","            res_array_full[i, j*l_Y:(j+1)*l_Y] = vals\n","    cols = [f'MMD: alpha={i}' for i in [0.1, 1.0, 5.0, 10.0]]+['Energy']\n","    if l_Y > 1:\n","        keys_uniq_Y = [\n","            str(i.cpu())+f' {j} obs' for i, j in zip(keys_uniq_Y, counts)]\n","    else:\n","        keys_uniq_Y = [str(i)+f' {j} obs' for i, j in zip(keys_uniq_Y, counts)]\n","    cols_Y = np.repeat(keys_uniq_Y, ncol, axis=0)\n","    cols_full = list(zip(*[cols_Y, np.tile(cols, l_Y)]))\n","    cols_full = pd.MultiIndex.from_tuples(\n","        cols_full, names=[\"Ys\", \"two_sample\"])\n","    res_array = pd.DataFrame(\n","        res_array, index=keys_gen_mtd, columns=cols).round(3)\n","    res_array_full = pd.DataFrame(\n","        res_array_full, index=keys_gen_mtd, columns=cols_full).round(3)\n","    res_array_full.index.name = 'gen_method'\n","    return [res_array, res_array_full.T]\n","\n","###################\n"]}]}