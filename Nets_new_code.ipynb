{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Nets_new_code.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_h3_4YVl77Qm"},"outputs":[],"source":["# from torch.nn.utils.parametrizations import spectral_norm\n","import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.autograd.functional import jacobian\n","from torch_geometric.nn import GCNConv, ChebConv, SAGEConv\n","from torch_geometric.nn import Sequential as Graph_Sequential\n","from torch.nn import Parameter\n","from L3net import GraphConv_Bases\n","import torch_geometric as pyg\n","# print(torch.__file__)\n","import pdb\n","from timeit import default_timer as timer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# IResNet\n","\n","\n","class input_tranpose(nn.Module):\n","    def __init__(self, dim0=1, dim1=2):\n","        super().__init__()\n","        self.dim0 = dim0\n","        self.dim1 = dim1\n","\n","    def forward(self, x):\n","        return torch.transpose(x, self.dim0, self.dim1)\n","\n","\n","class InvResBlock(nn.Module):\n","    '''\n","    First construct ResNet block as F_b=I+g, where\n","        g=W_i \\circ \\phi ... \\circ W_1, where the spectral norm of W_i is strictly smaller than 1 & phi can be chosen as ReLU, ELU, tanh, etc. as contractive nonlinearities\n","    '''\n","\n","    def __init__(self, C, model_args, version, A_):\n","        super().__init__()\n","        '''\n","        model_args=[c,dim]\n","            c: The spectral norm of the weight matrix. Too small choice seem to make things harder to train\n","            dim: Dimension of hidden representation\n","        '''\n","        dim, input_non_linear = model_args\n","        act = nn.ELU(inplace=True)\n","        layers = layers_append([], act, C, dim, C, version=version, A_=A_)\n","        self.bottleneck_block = nn.Sequential(*layers)\n","        self.actnorm = None\n","        self.C = C\n","\n","    def forward(self, x):\n","        n = int(x.shape[1]/self.C)\n","        x = x.reshape(x.shape[0], n, self.C)\n","        Fx = self.bottleneck_block(x)\n","        output = Fx+x\n","        output = output.flatten(start_dim=1)  # So it is in R^N times nC\n","        # The goal is so that the jacobian has shape nC-by-nC\n","        return [output, Fx]\n","\n","    def inverse(self, y, maxIter=100, eps=1e-7):\n","        x_pre = y\n","        for iter in range(maxIter):\n","            x_now = y-self.bottleneck_block(x_pre)\n","            diff = torch.linalg.norm(x_now-x_pre)\n","            if diff <= eps:\n","                break\n","            x_pre = x_now\n","        if self.actnorm is not None:\n","            x_now = self.actnorm.inverse(x_now)\n","        return x_now\n","\n","\n","class InvResBlock_Graph(nn.Module):\n","    '''\n","    First construct ResNet block as F_b=I+g, where\n","        g=W_tori \\circ \\phi ... \\circ W_1, where the spectral norm of W_i is strictly smaller than 1 & phi can be chosen as ReLU, ELU, tanh, etc. as contractive nonlinearities\n","    '''\n","\n","    def __init__(self, C, model_args, version='two_GCN_one_FC'):\n","        super().__init__()\n","        [dim, K] = model_args\n","        act = nn.ELU(inplace=True)\n","        # trans = input_tranpose(1, 2)\n","        layers = []\n","        layers = layers_append(layers, act, C, dim, C, K, version)\n","        self.bottleneck_block = Graph_Sequential(\n","            'x, edge_index, edge_weight', layers)\n","        # self.actnorm = ActNorm2D(C) # extremely slow\n","        self.actnorm = None\n","        self.C = C\n","\n","    def forward(self, x, edge_index, edge_weight):\n","        # HERE, need to reshape x first, as it is a flattened matrix of node features (so in dimension nC rather than n-X-C)\n","        # We assume each row is this flattened graph, and x\\in \\R^{N\\times nC}\n","        n = int(x.shape[1]/self.C)\n","        x = x.reshape(x.shape[0], n, self.C)\n","        Fx = self.bottleneck_block(x, edge_index, edge_weight)\n","        output = Fx+x\n","        output = output.flatten(start_dim=1)  # So it is in R^N times nC\n","        # The goal is so that the jacobian has shape nC-by-nC\n","        return [output, Fx]\n","\n","    def inverse(self, y, edge_index, edge_weight, maxIter=100, eps=1e-7):\n","        # Fixed point iteration to find the inverse\n","        x_pre = y\n","        for iter in range(maxIter):\n","            x_now = y-self.bottleneck_block(x_pre, edge_index, edge_weight)\n","            diff = torch.linalg.norm(x_now-x_pre)\n","            if diff <= eps:\n","                break\n","            x_pre = x_now\n","        if self.actnorm is not None:\n","            x_now = self.actnorm.inverse(x_now)\n","        return x_now\n","\n","\n","class InvResNet(nn.Module):\n","    '''\n","    Refer to https://github.com/jhjacobsen/invertible-resnet/blob/master/models/conv_iResNet.py, where they stack multiple blocks together\n","        Line 418 class conv_iResNet(nn.Module) stacks blocks together,\n","        Line 56 builds the block\n","    '''\n","\n","    def __init__(self, C, output_dim=1, nblocks=5, model_args=[0.9, 64, 3], graph=False, dim_inc=False, version='two_GCN_one_FC', A_=None):\n","        '''\n","            Output_dim: for classification\n","            num_nodes: number of graph nodes\n","            dim_inc: sometimes, if X \\in \\R^C lies in low dimension (e.g., graph node feature), inc. dimension and flow in \\R^C' maybe better in terms of avoiding pathological situations\n","        '''\n","        super().__init__()\n","        self.C = C\n","        dim, K = model_args[1], model_args[2]\n","        self.dim_inc = dim_inc\n","        # Actual dimension in which the distribution flows\n","        self.C_prime = 2*C if self.dim_inc else C\n","        self.blocks = nn.ModuleList([InvResBlock_Graph(self.C_prime, [dim, K], version) for b in range(\n","            nblocks)]) if graph else nn.ModuleList([InvResBlock(self.C_prime, [dim, b], version, A_) for b in range(nblocks)])\n","        if self.dim_inc:\n","            self.blocks = nn.ModuleList(\n","                [SimpleDimInc_block(C, self.C_prime), *self.blocks])\n","        self.fc = nn.Linear(self.C_prime, output_dim)\n","        if self.C_prime > 2:\n","            self.fc = nn.Linear(2, output_dim)\n","        self.reduce_factor = model_args[0]\n","        self.small_weights()\n","\n","    def forward(self, x, edge_index=None, edge_weight=None, logdet=True):\n","        start = 0\n","        if self.dim_inc:\n","            # Inc. dimension 1st\n","            n = int(x.shape[1]/self.C)\n","            x = x.reshape(x.shape[0], n, self.C)\n","            x = self.blocks[0](x).flatten(\n","                start_dim=1)  # So it is in R^N times nC\n","            start += 1\n","        log_det = 0\n","        transport_cost = 0\n","        for j, block in enumerate(self.blocks[start:]):\n","            if logdet:\n","                # In graph example, use the fast matrix log trace, but biased\n","                # TODO: improve it later with the Residual Flow paper idea\n","                if edge_index is not None:\n","                    det_block = torch.log(\n","                        torch.abs(torch.det(batch_jacobian(block, x, edge_index, edge_weight)))).sum()\n","                else:\n","                    det_block = torch.log(\n","                        torch.abs(torch.det(batch_jacobian(block, x)))).sum()\n","                log_det = log_det+det_block\n","            x, Fx = block(\n","                x, edge_index, edge_weight) if edge_index is not None else block(x)\n","            transport_cost += (torch.linalg.norm(Fx.flatten(start_dim=1),\n","                               dim=1)**2/2).sum()\n","        if logdet:\n","            return x, log_det, transport_cost\n","        else:\n","            return x\n","\n","    def inverse(self, y, edge_index=None, edge_weight=None, maxIter=50):\n","        with torch.no_grad():\n","            start = 1 if self.dim_inc else 0\n","            for block in reversed(self.blocks[start:]):\n","                y = block.inverse(\n","                    y, edge_index, edge_weight, maxIter) if edge_index is not None else block.inverse(y, maxIter)\n","            if self.dim_inc:\n","                y = self.blocks[0].inverse(y)\n","        return y\n","\n","    def classification(self, H):\n","        '''\n","            Yield a linear classifier\n","        '''\n","        return self.fc(H)\n","\n","    def small_weights(self):\n","        for name, W in self.named_parameters():\n","            if 'fc' not in name:\n","                with torch.no_grad():\n","                    # Of course, this is user-specified. It is just for initialization\n","                    # In fact, should not be too small, as it would make the transport cost too negligible\n","                    # And losses decay too slowly\n","                    # And the model more likely get non-invertible...\n","                    W.mul_(self.reduce_factor)\n","                W.requires_grad = True\n","\n","\n","# CGAN\n","\n","\n","class CGAN_net(nn.Module):\n","    '''\n","        Note, this is very similar to our IResBlock, but just we no longer concatenate multiple blocks\n","    '''\n","\n","    def __init__(self, C, dim, Y_dim=2, nblocks=10, classify=False, graph=True, version='two_L3_two_FC', A_=None):\n","        super().__init__()\n","        act = nn.ELU(inplace=True)\n","        full_layers = []\n","        trans = input_tranpose(1, 2)\n","        if nblocks > 1:\n","            for i in range(nblocks-1):\n","                if i == 0:\n","                    full_layers += layers_append([], act,\n","                                                 C, dim, dim, version=version, A_=A_)\n","                else:\n","                    full_layers += layers_append([], act,\n","                                                 dim, dim, dim, version=version, A_=A_)\n","                full_layers.append(trans)\n","                full_layers.append(pyg.nn.BatchNorm(dim))\n","                full_layers.append(trans)\n","            full_layers += layers_append([], act, dim,\n","                                         dim, C-Y_dim, version=version, A_=A_)\n","        else:\n","            full_layers = layers_append([], act, C,\n","                                        dim, C-Y_dim, version=version, A_=A_)\n","        self.graph = graph\n","        if self.graph:\n","            # e.g., ChebNet\n","            self.bottleneck_block = Graph_Sequential(\n","                'x, edge_index, edge_weight', full_layers)\n","        else:\n","            self.bottleneck_block = nn.Sequential(*full_layers)\n","        # self.actnorm = ActNorm2D(C) # extremely slow\n","        self.actnorm = None\n","        self.classify = classify\n","        if self.classify:\n","            last_layer = layers_append([], act,\n","                                       C-Y_dim, 32, 1, version='three_FC')\n","            self.D_output = nn.Sequential(*last_layer)\n","\n","    def forward(self, x, edge_index, edge_weight):\n","        if self.graph:\n","            output = self.bottleneck_block(x, edge_index, edge_weight)\n","        else:\n","            output = self.bottleneck_block(x)\n","        if self.classify:\n","            # For the min-max GAN\n","            offset = 1e-4\n","            output = torch.nn.Sigmoid()(self.D_output(output))\n","            offset_vec = torch.zeros(output.size()).to(device)\n","            if (output < offset).sum() > 0:\n","                offset_vec[output < offset] = (\n","                    offset - output[output < offset]).clone().detach()\n","            if (output > 1-offset).sum() > 0:\n","                offset_vec[output > 1\n","                           - offset] = -(output[output > 1-offset]-(1-offset)).clone().detach()\n","            return output+offset_vec\n","            # # For Wasserstain GAN\n","            # return self.D_output(output)\n","        else:\n","            return output\n","\n","    def set_requires_grad(self, TorF):\n","        for param in self.parameters():\n","            param.requires_grad = TorF\n","\n","# # RNVP\n","#\n","#\n","# class R_NVP(nn.Module):\n","#     def __init__(self, d, c=0, hidden=64, version='three_FC'):\n","#         super().__init__()\n","#         self.d, self.c = d, c\n","#         # act = nn.ELU(inplace=True)\n","#         act = nn.LeakyReLU(inplace=True)\n","#         k = int((d+c)/2)\n","#         self.k = k\n","#         layers1 = layers_append([], act, d+c-k, hidden,\n","#                                 k, version=version)\n","#         layers2 = layers_append([], act, k, hidden,\n","#                                 d+c-k, version=version)\n","#         self.s1 = nn.Sequential(*layers1)\n","#         self.t1 = nn.Sequential(*layers1.copy())\n","#         self.s2 = nn.Sequential(*layers2)\n","#         self.t2 = nn.Sequential(*layers2.copy())\n","#\n","#     def forward(self, X, Y):\n","#         # Y denotes the condition variable\n","#         if self.c > 0:\n","#             # 1st block, takes in conditional variable\n","#             input = torch.cat([X, Y], dim=-1)\n","#         else:\n","#             input = X\n","#         x1, x2 = input[:, :self.k], input[:, self.k:]\n","#         s1_out = self.s1(x2)\n","#         v1 = x1*torch.exp(s1_out)+self.t1(x2)\n","#         s2_out = self.s2(v1)\n","#         v2 = x2*torch.exp(s2_out)+self.t2(v1)\n","#         v_tot = torch.cat([v1, v2], dim=-1)\n","#         log_jacob = s1_out.sum(-1)+s2_out.sum(-1)\n","#         return v_tot, log_jacob\n","#\n","#     def inverse(self, Z, Y):\n","#         if self.c1 > 0:\n","#             # Last block, takes in conditional variable\n","#             input = torch.cat([Z, Y], dim=-1)\n","#         else:\n","#             input = Z\n","#         z1, z2 = input[:, :self.k], input[:, self.k:]\n","#         x2_hat = (z2 - self.t2(z1)) * torch.exp(-self.s2(z1))\n","#         x1_hat = (z1 - self.t1(x2_hat)) * torch.exp(-self.s1(x2_hat))\n","#         return torch.cat([x1_hat, x2_hat], -1)\n","#\n","#\n","# class stacked_NVP(nn.Module):\n","#     # NOTE: this is very easy to be non-invertible...\n","#     def __init__(self, d, c, hidden, num_b, version='one_Cheb_two_FC'):\n","#         super().__init__()\n","#         self.d = d\n","#         bijectors = [R_NVP(d, c, hidden=hidden, version=version)]\n","#         for _ in range(num_b-1):\n","#             bijectors += [R_NVP(d+c, 0, hidden=hidden, version=version)]\n","#         self.bijectors = nn.ModuleList(bijectors)\n","#         self.small_weight(factor=0.15)\n","#\n","#     def forward(self, X, Y):\n","#         log_jacobs = []\n","#         for bijector in self.bijectors:\n","#             X, log_jac = bijector(X, Y)\n","#             log_jacobs.append(log_jac)\n","#         return X[:, :self.d], sum(log_jacobs)\n","#\n","#     def inverse(self, Z, Y):\n","#         self.bijectors[-1].c1 = 1  # place holder\n","#         for k in range(len(self.bijectors)-1):\n","#             self.bijectors[k].c1 = 0\n","#         with torch.no_grad():\n","#             for bijector in reversed(self.bijectors):\n","#                 Z = bijector.inverse(Z, Y)\n","#         return Z[:, :self.d]\n","#\n","#     def small_weight(self, factor):\n","#         # Scale parameter, as o/w the torch.exp() can make things explode\n","#         for W in self.parameters():\n","#             with torch.no_grad():\n","#                 W.mul_(factor)\n","#             W.requires_grad = True\n","\n","# class R_NVP(nn.Module):\n","#     def __init__(self, d, k, c, hidden, version='three_FC'):\n","#         super().__init__()\n","#         self.d, self.k = d, k\n","#         act = nn.ELU(inplace=True)\n","#         layers1 = layers_append([], act, d-k+c, hidden,\n","#                                 k, version=version)\n","#         layers2 = layers_append([], act, k+c, hidden,\n","#                                 d-k, version=version)\n","#         self.s1 = nn.Sequential(*layers1)\n","#         self.t1 = nn.Sequential(*layers1.copy())\n","#         self.s2 = nn.Sequential(*layers2)\n","#         self.t2 = nn.Sequential(*layers2.copy())\n","\n","#     def forward(self, X, Y):\n","#         # Y denotes the condition variable\n","#         x1, x2 = X[:, :self.k], X[:, self.k:]\n","#         x2_long = torch.cat([x2, Y], dim=-1)\n","#         s1_out, t1_out = self.s1(x2_long), self.t1(x2_long)\n","#         v1 = x1*torch.exp(s1_out)+t1_out\n","#         s2_out, t2_out = self.s2(\n","#             torch.cat([v1, Y], dim=-1)), self.t2(torch.cat([v1, Y], dim=-1))\n","#         v2 = x2*torch.exp(s2_out)+t2_out\n","#         v_tot = torch.cat([v1, v2], dim=-1)\n","#         log_jacob = s1_out.sum(-1)+s2_out.sum(-1)\n","#         return v_tot, log_jacob\n","\n","#     def inverse(self, Z, Y):\n","#         z1, z2 = Z[:, :self.k], Z[:, self.k:]\n","#         z1_long = torch.cat([z1, Y], dim=-1)\n","#         x2_hat = (z2 - self.t2(z1_long)) * \\\n","#             torch.exp(-self.s2(z1_long))\n","#         x1_hat = (z1 - self.t1(torch.cat([x2_hat, Y], dim=-1))) * \\\n","#             torch.exp(-self.s1(torch.cat([x2_hat, Y],\n","#                       dim=-1)))\n","#         return torch.cat([x1_hat, x2_hat], -1)\n","\n","\n","# class stacked_NVP(nn.Module):\n","#     def __init__(self, d, k, c, hidden, num_b, version='one_Cheb_two_FC'):\n","#         super().__init__()\n","#         self.bijectors = nn.ModuleList([\n","#             R_NVP(d, k, c, hidden=hidden, version=version) for _ in range(num_b)\n","#         ])\n","#         self.small_weight(factor=0.5)\n","\n","#     def forward(self, X, Y):\n","#         log_jacobs = []\n","#         for bijector in self.bijectors:\n","#             X, log_jac = bijector(X, Y)\n","#             log_jacobs.append(log_jac)\n","#         return X, sum(log_jacobs)\n","\n","#     def inverse(self, z, Y):\n","#         with torch.no_grad():\n","#             for bijector in reversed(self.bijectors):\n","#                 z = bijector.inverse(z, Y)\n","#         return z\n","\n","#     def small_weight(self, factor):\n","#         # Scale parameter, as o/w the torch.exp() can make things explode\n","#         for W in self.parameters():\n","#             with torch.no_grad():\n","#                 W.mul_(factor)\n","#             W.requires_grad = True\n","\n","# Append net:\n","\n","\n","def layers_append(layers, act, C, dim, C1, K=3, version='one_Cheb_two_FC', A_=None):\n","    if version == 'one_GCN_one_FC':\n","        layers.append((GCNConv(C, dim), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'one_GCN_two_FC':\n","        layers.append((GCNConv(C, dim), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'two_GCN_one_FC':\n","        layers.append((GCNConv(C, dim), 'x, edge_index, edge_weight -> x'))\n","        # layers.append(pyg.nn.BatchNorm(dim)) # Some issues existed, IDK why\n","        layers.append(act)\n","        layers.append(\n","            (GCNConv(dim, dim), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'one_Cheb_two_FC':\n","        layers.append(\n","            (ChebConv(C, dim, K=K), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, dim))\n","        # # NOTE, Batchnorm makes invertibility somehow not hold, but transpose etc. works\n","        # layers.append(trans)\n","        # layers.append(pyg.nn.BatchNorm(dim))\n","        # layers.append(trans)\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'one_Cheb_three_FC':\n","        layers.append(\n","            (ChebConv(C, dim, K=K), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'two_Cheb_two_FC':\n","        layers.append(\n","            (ChebConv(C, dim, K=K), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(\n","            (ChebConv(dim, dim, K=K), 'x, edge_index, edge_weight -> x'))\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, dim))\n","        # # NOTE, Batchnorm makes invertibility somehow not hold, but transpose etc. works\n","        # layers.append(trans)\n","        # layers.append(pyg.nn.BatchNorm(dim))\n","        # layers.append(trans)\n","        layers.append(act)\n","        layers.append(torch.nn.Linear(dim, C1))\n","    if version == 'one_Cheb':\n","        layers.append(\n","            (ChebConv(C, dim, K=K), 'x, edge_index, edge_weight -> x'))\n","    if version == 'three_FC':\n","        layers.append(nn.Linear(C, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, C1))\n","    if version == 'four_FC':\n","        layers.append(nn.Linear(C, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, C1))\n","    # For L3net:\n","    trans = input_tranpose(1, 2)\n","    order_list = [1]\n","    if A_ is not None:\n","        if A_.shape[0] == 3:\n","            # The simulation example\n","            order_list = [0, 1, 2]\n","        if A_.shape[0] == 10:\n","            # Solar data\n","            order_list = [1, 2]  # Two bases, with 1 & 2 hop neighbors\n","        if A_.shape[0] == 20 or A_.shape[0] == 15:\n","            # Traffic data\n","            # order_list = [1, 2, 2]  # Two bases, with 1 & 2 hop neighbors\n","            order_list = [0, 1, 2]\n","    if version == 'one_L3_two_FC':\n","        layers.append(trans)\n","        layers.append(GraphConv_Bases(C, dim, A_, order_list=order_list))\n","        layers.append(trans)\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, C1))\n","    if version == 'one_L3_three_FC':\n","        layers.append(trans)\n","        layers.append(GraphConv_Bases(C, dim, A_, order_list=order_list))\n","        layers.append(trans)\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, C1))\n","    if version == 'two_L3_two_FC':\n","        layers.append(trans)\n","        layers.append(GraphConv_Bases(C, dim, A_, order_list=order_list))\n","        layers.append(GraphConv_Bases(dim, dim, A_, order_list=order_list))\n","        layers.append(trans)\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, dim))\n","        layers.append(act)\n","        layers.append(nn.Linear(dim, C1))\n","    if version == 'one_L3':\n","        layers.append(trans)\n","        order_list = [0, 1, 2]\n","        layers.append(GraphConv_Bases(C, dim, A_, order_list=order_list))\n","        layers.append(trans)\n","    return layers\n","# Small nets\n","\n","\n","class SmallGenNet(nn.Module):\n","    '''\n","        Yield the conditional mean of the base distribution using one-hot encoded response Y\n","    '''\n","\n","    def __init__(self, Y_dim, C):\n","        super().__init__()\n","        self.fc = nn.Linear(Y_dim, C)\n","\n","    def forward(self, Y):\n","        return self.fc(Y)\n","\n","\n","# Helpers\n","def batch_jacobian(func, x, edge_index=None, edge_weight=None):\n","    # Basically apply the jacobian function on each sample in the batch\n","    # x in shape (Batch, Length)\n","    def _func_sum(x):\n","        if edge_index is not None:\n","            return func(x, edge_index, edge_weight)[0].sum(dim=0)\n","        else:\n","            return func(x)[0].sum(dim=0)\n","    return jacobian(_func_sum, x, create_graph=True).permute(1, 0, 2)\n","\n","\n","#######\n"]}]}